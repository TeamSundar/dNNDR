{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596707156448",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from functions import MODEL\n",
    "\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import max_norm\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Activation, Concatenate, Layer\n",
    "from keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D, LSTM\n",
    "import keras.backend as K\n",
    "\n",
    "obj = MODEL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTI_index=pd.read_csv('data/DTI_index.csv')[['target','drug','IC50','unit','activity','target_uniprot']]\n",
    "target_seq=pd.read_csv('data/target_seq.csv')[['target_uniprot','target_chembl','seq']]\n",
    "drug_smiles=pd.read_csv('data/drug_smiles.csv')[['drug','smile','seq_char_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(61624, 6)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "DTI_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indics into train/test\n",
    "train_indices, test_indices = obj.split(DTI_index, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(61427, 1) (61427, 1) (15357, 1) (15357, 1) (61427, 1) (15357, 1)\n"
    }
   ],
   "source": [
    "# Train and test data\n",
    "train_target = DTI_index.loc[train_indices][['target_uniprot']]\n",
    "train_drug = DTI_index.loc[train_indices][['drug']]\n",
    "test_target = DTI_index.loc[test_indices][['target_uniprot']]\n",
    "test_drug = DTI_index.loc[test_indices][['drug']]\n",
    "\n",
    "# Labels\n",
    "train_y = DTI_index.loc[train_indices][['activity']]\n",
    "test_y = DTI_index.loc[test_indices][['activity']]\n",
    "\n",
    "print(train_target.shape, train_drug.shape, test_target.shape, test_drug.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 61427/61427 [00:35<00:00, 1729.82it/s]\n"
    }
   ],
   "source": [
    "def addSeq():\n",
    "    # Add sequence to corresponding target IDs\n",
    "    seq_target = []\n",
    "    for target in tqdm(train_target['target_uniprot']):\n",
    "        try:\n",
    "            seq_target.append(target_seq[target_seq['target_uniprot']==target]['seq'].values[0])\n",
    "        except:\n",
    "            print(target)\n",
    "    train_target['seq'] = seq_target\n",
    "\n",
    "    # Add smile strings to corresponding drug IDs\n",
    "    seq_drug = []\n",
    "    for drug in tqdm(train_drug['drug']):\n",
    "        try:\n",
    "            seq_drug.append(drug_smiles[drug_smiles['drug']==drug]['smile'].values[0])\n",
    "        except:\n",
    "            print(target)\n",
    "    train_drug['seq'] = seq_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.subplot(1, 1, 1)\n",
    "# obj.plot_seq_count(drug_smiles, 'Train')\n",
    "\n",
    "# code_freq = obj.get_code_freq(target_seq['seq'], 'Train')\n",
    "# plt.subplot(1, 1, 1)\n",
    "# obj.plot_code_freq(code_freq, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}\nTarget Dict Length: 20\n{'[': 1, 'l': 2, 'I': 3, 'K': 4, 'O': 5, 'B': 6, '6': 7, 'F': 8, ')': 9, '7': 10, '3': 11, 'a': 12, 'P': 13, 'c': 14, ']': 15, '/': 16, '(': 17, 'S': 18, '1': 19, 'e': 20, 'i': 21, 'Z': 22, '8': 23, 'H': 24, '+': 25, 'r': 26, '4': 27, 'L': 28, '5': 29, 'N': 30, 'A': 31, '.': 32, '9': 33, 'C': 34, '@': 35, '2': 36, 'o': 37, '#': 38, 'n': 39, '\\\\': 40, '-': 41, 's': 42, '=': 43}\nDrug dict Length: 43\n"
    }
   ],
   "source": [
    "# Encode amino acides and smile characters\n",
    "codes_target = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "char_dict_target = obj.create_dict(codes_target)\n",
    "\n",
    "codes_drug = [char for char in ''.join(set(''.join(drug_smiles['smile'].values)))]\n",
    "char_dict_drug = obj.create_dict(codes_drug)\n",
    "\n",
    "print(char_dict_target)\n",
    "print(\"Target Dict Length:\", len(char_dict_target))\n",
    "\n",
    "print(char_dict_drug)\n",
    "print(\"Drug dict Length:\", len(char_dict_drug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encode_target = obj.integer_encoding(train_target, char_dict_target) \n",
    "train_encode_drug = obj.integer_encoding(train_drug, char_dict_drug) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((61427, 1000), (61427, 1000))"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# padding sequences\n",
    "max_length = 1000\n",
    "train_pad_target = pad_sequences(train_encode_target, maxlen=max_length, padding='post', truncating='post')\n",
    "train_pad_drug = pad_sequences(train_encode_drug, maxlen=max_length, padding='post', truncating='post')\n",
    "train_pad_target.shape, train_pad_drug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((61427, 1000, 21), (61427, 1000, 44))"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# One hot encoding of sequences\n",
    "train_ohe_target = to_categorical(train_pad_target)\n",
    "train_ohe_drug = to_categorical(train_pad_drug)\n",
    "train_ohe_target.shape, train_ohe_drug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(61427,)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# label/integer encoding output variable: (y)\n",
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(train_y['activity'].tolist())\n",
    "y_train_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(61427, 3)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# One hot encoding of outputs\n",
    "y_train = to_categorical(y_train_le)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention class\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_11 (InputLayer)           [(None, 1000)]       0                                            \n__________________________________________________________________________________________________\ninput_12 (InputLayer)           [(None, 1000)]       0                                            \n__________________________________________________________________________________________________\nembedding_10 (Embedding)        (None, 1000, 128)    2688        input_11[0][0]                   \n__________________________________________________________________________________________________\nembedding_11 (Embedding)        (None, 1000, 128)    5632        input_12[0][0]                   \n__________________________________________________________________________________________________\nconv1d_10 (Conv1D)              (None, 1000, 32)     12320       embedding_10[0][0]               \n__________________________________________________________________________________________________\nconv1d_11 (Conv1D)              (None, 1000, 32)     12320       embedding_11[0][0]               \n__________________________________________________________________________________________________\nmax_pooling1d_10 (MaxPooling1D) (None, 500, 32)      0           conv1d_10[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling1d_11 (MaxPooling1D) (None, 500, 32)      0           conv1d_11[0][0]                  \n__________________________________________________________________________________________________\nbidirectional_10 (Bidirectional (None, 500, 64)      16640       max_pooling1d_10[0][0]           \n__________________________________________________________________________________________________\nbidirectional_11 (Bidirectional (None, 500, 64)      16640       max_pooling1d_11[0][0]           \n__________________________________________________________________________________________________\nflatten_6 (Flatten)             (None, 32000)        0           bidirectional_10[0][0]           \n__________________________________________________________________________________________________\nflatten_7 (Flatten)             (None, 32000)        0           bidirectional_11[0][0]           \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 64000)        0           flatten_6[0][0]                  \n                                                                 flatten_7[0][0]                  \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 1024)         65537024    concatenate_5[0][0]              \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 512)          524800      dense_15[0][0]                   \n__________________________________________________________________________________________________\ndense_17 (Dense)                (None, 3)            1539        dense_16[0][0]                   \n==================================================================================================\nTotal params: 66,129,603\nTrainable params: 66,129,603\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Model Architecture\n",
    "input_target = Input(shape=(1000,))\n",
    "emb_target = Embedding(21, 128, input_length=max_length)(input_target) \n",
    "conv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\n",
    "pool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\n",
    "att_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\n",
    "#att_out_target = attention()(att_in_target)\n",
    "flat_1_target = Flatten()(att_in_target)\n",
    "\n",
    "# softmax classifier\n",
    "#x_output_target = Dense(3, activation='softmax')(att_in_target)\n",
    "\n",
    "input_drug = Input(shape=(1000,))\n",
    "emb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \n",
    "conv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\n",
    "pool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\n",
    "att_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\n",
    "#att_out_drug = attention()(att_in_drug)\n",
    "flat_1_drug = Flatten()(att_in_drug)\n",
    "\n",
    "concat = Concatenate()([flat_1_target,flat_1_drug])\n",
    "\n",
    "dense_1 = Dense(1024, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "dense_2 = Dense(512, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "\n",
    "# softmax classifier\n",
    "x_output = Dense(3, activation='softmax')(dense_2)\n",
    "\n",
    "model1 = Model(inputs=[input_target, input_drug], outputs=x_output)\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model1, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n384/384 [==============================] - 2477s 6s/step - loss: 2.4581 - accuracy: 0.5651 - val_loss: 1.4798 - val_accuracy: 0.6119\nEpoch 2/100\n384/384 [==============================] - 2472s 6s/step - loss: 1.2031 - accuracy: 0.6336 - val_loss: 1.0047 - val_accuracy: 0.6445\nEpoch 3/100\n384/384 [==============================] - 2504s 7s/step - loss: 0.8617 - accuracy: 0.6769 - val_loss: 0.7911 - val_accuracy: 0.6831\nEpoch 4/100\n384/384 [==============================] - 2518s 7s/step - loss: 0.6875 - accuracy: 0.7230 - val_loss: 0.6750 - val_accuracy: 0.7210\nEpoch 5/100\n384/384 [==============================] - 1838s 5s/step - loss: 0.5825 - accuracy: 0.7650 - val_loss: 0.6509 - val_accuracy: 0.7329\nEpoch 6/100\n384/384 [==============================] - 1388s 4s/step - loss: 0.5086 - accuracy: 0.7978 - val_loss: 0.6256 - val_accuracy: 0.7503\nEpoch 7/100\n384/384 [==============================] - 1402s 4s/step - loss: 0.4443 - accuracy: 0.8248 - val_loss: 0.6323 - val_accuracy: 0.7522\nEpoch 8/100\n384/384 [==============================] - 1296s 3s/step - loss: 0.3953 - accuracy: 0.8457 - val_loss: 0.6309 - val_accuracy: 0.7645\nEpoch 9/100\n246/384 [==================>...........] - ETA: 6:50 - loss: 0.3412 - accuracy: 0.8678"
    },
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[64000,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Adam/Adam/update_18/ResourceApplyAdam (defined at <ipython-input-34-c11ed5ffe4d8>:5) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_62068]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c11ed5ffe4d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     ))\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mf:\\Projects\\20202507_dNNDR\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64000,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Adam/Adam/update_18/ResourceApplyAdam (defined at <ipython-input-34-c11ed5ffe4d8>:5) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_62068]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "histories.append(model1.fit(\n",
    "    [train_pad_target, train_pad_drug], y_train,\n",
    "    epochs=100, batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bba9c91cc250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot model history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Plot model history\n",
    "obj.plot_history(histories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "import os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\nfrom tqdm import tqdm\nfrom functions import MODEL\n\nfrom collections import Counter\nfrom prettytable import PrettyTable\nfrom IPython.display import Image\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Model\nfrom keras.regularizers import l2\nfrom keras.constraints import max_norm\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Input, Dense, Dropout, Flatten, Activation, Concatenate, Layer\nfrom keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\nfrom keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D, LSTM\nimport keras.backend as K\n\nobj = MODEL()\n# Import and process level 2 data\ndrug_smiles = pd.read_csv('data/smiles.csv')[['drug','smile']]  # Drug index dataframe\ndrug_smiles['seq_char_count'] = drug_smiles['smile'].apply(lambda x: len(str(x)))   # Add column with character count\ndrug_smiles = drug_smiles[drug_smiles['smile'].notnull()]   # Remove drugs with no smile strings\nDTI_index = pd.read_csv('data/final_clean_DTI.csv')[['target','drug','IC50','unit']]\nDTI_index = DTI_index[DTI_index['drug'].isin(drug_smiles['drug'].tolist())]     # Exclude drugs for which smils are not available\nDTI_index = DTI_index.reset_index(drop=True)\nmapping = pd.read_csv('data/chembl2uniprot.txt', header=None, sep='\\t')\nmapping = mapping[mapping[0].isin(DTI_index['target'].unique())]    # Select targets that are present in data (DTI_index)\ntargets = os.listdir('data/fasta_968')  # List all target fasta files\n\n# Categorize drug-target pairs by IC50 values (Make labels)\nact = []\nbct = []\nfor i in tqdm(range(DTI_index.shape[0])):\n    if DTI_index['IC50'][i]<=0.1:\n        act.append('active')\n    elif DTI_index['IC50'][i]>0.1 and DTI_index['IC50'][i]<=30:\n        act.append('intermediate')\n    elif DTI_index['IC50'][i]>30:\n        act.append('inactive')\n    bct.append(mapping[mapping[0]==DTI_index['target'][i]][1].values[0])\n    \nDTI_index['activity'] = act\nDTI_index['target_uniprot'] = bct\n# Fetch fasta sequences form files and create target index dataframe\ndef fetchFasta(targets):\n    target_seq = pd.DataFrame(columns=['target_uniprot','target_chembl','seq'])\n    for fasta in tqdm(targets):\n        if fasta.split('.')[0] in mapping[1].tolist():\n            f = open('data/fasta_968/'+fasta,'r')\n            lines = \"\".join(line.strip() for line in f.readlines()[1:])\n            dict = {'target_uniprot':fasta.split('.')[0],'target_chembl':mapping[(mapping[1]==fasta.split('.')[0])][0].values[0], 'seq':lines}\n            target_seq = target_seq.append(dict, True)\n            f.close()\n    return target_seq\n\ntarget_seq = fetchFasta(targets)\n# Length of sequence in train data.\n#target_seq['seq_char_count'] = target_seq['seq'].apply(lambda : len(x))\n# Split indics into train/test\ntrain_indices, test_indices = obj.split(DTI_index, 8)\n# Train and test data\ntrain_target = DTI_index.loc[train_indices][['target_uniprot']]\ntrain_drug = DTI_index.loc[train_indices][['drug']]\ntest_target = DTI_index.loc[test_indices][['target_uniprot']]\ntest_drug = DTI_index.loc[test_indices][['drug']]\n\n# Labels\ntrain_y = DTI_index.loc[train_indices][['activity']]\ntest_y = DTI_index.loc[test_indices][['activity']]\n\nprint(train_target.shape, train_drug.shape, test_target.shape, test_drug.shape, train_y.shape, test_y.shape)\n# Add uniprot IDs to data index\nseq = []\nfor target in tqdm(train_target['target_uniprot']):\n    try:\n        seq.append(target_seq[target_seq['target_uniprot']==target]['seq'].values[0])\n    except:\n        print(target)\ntrain_target['seq'] = seq\n# Add smile strings to labels\nseq = []\nfor drug in tqdm(train_drug['drug']):\n    try:\n        seq.append(drug_smiles[drug_smiles['drug']==drug]['smile'].values[0])\n    except:\n        print(target)\ntrain_drug['seq'] = seq\n\n# plt.subplot(1, 1, 1)\n# obj.plot_seq_count(drug_smiles, 'Train')\n\n# code_freq = obj.get_code_freq(target_seq['seq'], 'Train')\n# plt.subplot(1, 1, 1)\n# obj.plot_code_freq(code_freq, 'Train')\n# Encode amino acides and smile characters\ncodes_target = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\nchar_dict_target = obj.create_dict(codes_target)\n\ncodes_drug = [char for char in ''.join(set(''.join(drug_smiles['smile'].values)))]\nchar_dict_drug = obj.create_dict(codes_drug)\n\nprint(char_dict_target)\nprint(\"Target Dict Length:\", len(char_dict_target))\n\nprint(char_dict_drug)\nprint(\"Drug dict Length:\", len(char_dict_drug))\ntrain_encode_target = obj.integer_encoding(train_target, char_dict_target) \ntrain_encode_drug = obj.integer_encoding(train_drug, char_dict_drug)\n# padding sequences\nmax_length = 1000\ntrain_pad_target = pad_sequences(train_encode_target, maxlen=max_length, padding='post', truncating='post')\ntrain_pad_drug = pad_sequences(train_encode_drug, maxlen=max_length, padding='post', truncating='post')\ntrain_pad_target.shape, train_pad_drug.shape\n# One hot encoding of sequences\ntrain_ohe_target = to_categorical(train_pad_target)\ntrain_ohe_drug = to_categorical(train_pad_drug)\ntrain_ohe_target.shape, train_ohe_drug.shape\n# label/integer encoding output variable: (y)\nle = LabelEncoder()\ny_train_le = le.fit_transform(train_y['activity'].tolist())\ny_train_le.shape\n# One hot encoding of outputs\ny_train = to_categorical(y_train_le)\ny_train.shape\n# Attention class\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()\n# Model Architecture\ninput_target = Input(shape=(1000,))\nemb_target = Embedding(21, 128, input_length=max_length)(input_target) \nconv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\npool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\natt_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\natt_out_target = attention()(att_in_target)\nflat_1_target = Flatten()(att_out_target)\n\n# softmax classifier\n#x_output_target = Dense(3, activation='softmax')(att_in_target)\n\ninput_drug = Input(shape=(1000,))\nemb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \nconv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\npool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\natt_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\natt_out_drug = attention()(att_in_drug)\nflat_1_drug = Flatten()(att_out_drug)\n\nconcat = Concatenate()([flat_1_target,flat_1_drug])\n\ndense_1 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(concat)\ndense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n\n# softmax classifier\nx_output = Dense(3, activation='softmax')(dense_2)\n\nmodel1 = Model(inputs=[input_target, input_drug], outputs=x_output)\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel1.summary()\n# Early Stopping\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistories = []\nhistories.append(model1.fit(\n    [train_pad_target, train_pad_drug], y_train,\n    epochs=100, batch_size=128,\n    validation_split=0.2,\n    callbacks=[es]\n    ))\n# Model Architecture\ninput_target = Input(shape=(1000,))\nemb_target = Embedding(21, 256, input_length=max_length)(input_target) \nconv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\npool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\natt_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\natt_out_target = attention()(att_in_target)\nflat_1_target = Flatten()(att_out_target)\n\n# softmax classifier\n#x_output_target = Dense(3, activation='softmax')(att_in_target)\n\ninput_drug = Input(shape=(1000,))\nemb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \nconv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\npool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\natt_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\natt_out_drug = attention()(att_in_drug)\nflat_1_drug = Flatten()(att_out_drug)\n\nconcat = Concatenate()([flat_1_target,flat_1_drug])\n\ndense_1 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(concat)\ndense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n\n# softmax classifier\nx_output = Dense(3, activation='softmax')(dense_2)\n\nmodel1 = Model(inputs=[input_target, input_drug], outputs=x_output)\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel1.summary()\n# Early Stopping\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistories = []\nhistories.append(model1.fit(\n    [train_pad_target, train_pad_drug], y_train,\n    epochs=100, batch_size=128,\n    validation_split=0.2,\n    callbacks=[es]\n    ))\n# Model Architecture\ninput_target = Input(shape=(1000,))\nemb_target = Embedding(21, 256, input_length=max_length)(input_target) \nconv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\npool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\natt_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\n#att_out_target = attention()(att_in_target)\n#flat_1_target = Flatten()(att_out_target)\n\n# softmax classifier\n#x_output_target = Dense(3, activation='softmax')(att_in_target)\n\ninput_drug = Input(shape=(1000,))\nemb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \nconv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\npool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\natt_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\n#att_out_drug = attention()(att_in_drug)\n#flat_1_drug = Flatten()(att_out_drug)\n\nconcat = Concatenate()([att_in_target,att_in_drug])\n\ndense_1 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(concat)\ndense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n\n# softmax classifier\nx_output = Dense(3, activation='softmax')(dense_2)\n\nmodel1 = Model(inputs=[input_target, input_drug], outputs=x_output)\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel1.summary()\n# Early Stopping\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistories = []\nhistories.append(model1.fit(\n    [train_pad_target, train_pad_drug], y_train,\n    epochs=100, batch_size=128,\n    validation_split=0.2,\n    callbacks=[es]\n    ))\ndense_2\n# Model Architecture\ninput_target = Input(shape=(1000,))\nemb_target = Embedding(21, 128, input_length=max_length)(input_target) \nconv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\npool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\natt_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\n#att_out_target = attention()(att_in_target)\n#flat_1_target = Flatten()(att_out_target)\n\n# softmax classifier\n#x_output_target = Dense(3, activation='softmax')(att_in_target)\n\ninput_drug = Input(shape=(1000,))\nemb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \nconv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\npool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\natt_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\n#att_out_drug = attention()(att_in_drug)\n#flat_1_drug = Flatten()(att_out_drug)\n\nconcat = Concatenate()([att_in_target,att_in_drug])\n\ndense_1 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(concat)\ndense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n\n# softmax classifier\nx_output = Dense(3, activation='softmax')(dense_2)\n\nmodel1 = Model(inputs=[input_target, input_drug], outputs=x_output)\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel1.summary()\n# Early Stopping\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistories = []\nhistories.append(model1.fit(\n    [train_pad_target, train_pad_drug], y_train,\n    epochs=100, batch_size=128,\n    validation_split=0.2,\n    callbacks=[es]\n    ))\n# Model Architecture\ninput_target = Input(shape=(1000,))\nemb_target = Embedding(21, 128, input_length=max_length)(input_target) \nconv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\npool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\natt_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\n#att_out_target = attention()(att_in_target)\nflat_1_target = Flatten()(att_in_target)\n\n# softmax classifier\n#x_output_target = Dense(3, activation='softmax')(att_in_target)\n\ninput_drug = Input(shape=(1000,))\nemb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \nconv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\npool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\natt_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\n#att_out_drug = attention()(att_in_drug)\nflat_1_drug = Flatten()(att_in_drug)\n\nconcat = Concatenate()([flat_1_target,flat_1_drug])\n\ndense_1 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(concat)\ndense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n\n# softmax classifier\nx_output = Dense(3, activation='softmax')(dense_2)\n\nmodel1 = Model(inputs=[input_target, input_drug], outputs=x_output)\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel1.summary()\n# Early Stopping\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistories = []\nhistories.append(model1.fit(\n    [train_pad_target, train_pad_drug], y_train,\n    epochs=100, batch_size=128,\n    validation_split=0.2,\n    callbacks=[es]\n    ))\n# Model Architecture\ninput_target = Input(shape=(1000,))\nemb_target = Embedding(21, 128, input_length=max_length)(input_target) \nconv_target_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_target)\npool_target_1 = MaxPooling1D(pool_size=2)(conv_target_1)\natt_in_target = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_target_1)\n#att_out_target = attention()(att_in_target)\nflat_1_target = Flatten()(att_in_target)\n\n# softmax classifier\n#x_output_target = Dense(3, activation='softmax')(att_in_target)\n\ninput_drug = Input(shape=(1000,))\nemb_drug = Embedding(44, 128, input_length=max_length)(input_drug) \nconv_drug_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_drug)\npool_drug_1 = MaxPooling1D(pool_size=2)(conv_drug_1)\natt_in_drug = Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), return_sequences=True, recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(pool_drug_1)\n#att_out_drug = attention()(att_in_drug)\nflat_1_drug = Flatten()(att_in_drug)\n\nconcat = Concatenate()([flat_1_target,flat_1_drug])\n\ndense_1 = Dense(1024, activation = 'relu',kernel_initializer='glorot_normal')(concat)\ndense_2 = Dense(512, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n\n# softmax classifier\nx_output = Dense(3, activation='softmax')(dense_2)\n\nmodel1 = Model(inputs=[input_target, input_drug], outputs=x_output)\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel1.summary()\n# Early Stopping\nes = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\nhistories = []\nhistories.append(model1.fit(\n    [train_pad_target, train_pad_drug], y_train,\n    epochs=100, batch_size=128,\n    validation_split=0.2,\n    callbacks=[es]\n    ))\n# Plot model history\nobj.plot_history(histories[0])\nhistories\nhistory\n"
    }
   ],
   "source": []
  }
 ]
}